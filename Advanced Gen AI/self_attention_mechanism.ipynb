{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Simulated input (3 words, 4 embedding dimensions)\n",
        "Q = torch.rand((3, 4))\n",
        "K = torch.rand((3, 4))\n",
        "V = torch.rand((3, 4))\n",
        "\n",
        "# Compute attention scores (Q @ K^T)\n",
        "attention_scores = Q @ K.T\n",
        "\n",
        "# Normalize using Softmax\n",
        "attention_weights = F.softmax(attention_scores, dim=-1)\n",
        "\n",
        "# Compute final attention output\n",
        "output = attention_weights @ V\n",
        "\n",
        "print(\"Attention Weights:\\n\", attention_weights)\n",
        "print(\"\\nSelf-Attention Output:\\n\", output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7PRzrSiiCO8",
        "outputId": "548a7a57-db0b-48ce-a989-7fd4b22ed417"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Weights:\n",
            " tensor([[0.2668, 0.4338, 0.2994],\n",
            "        [0.2735, 0.4257, 0.3008],\n",
            "        [0.2932, 0.4504, 0.2563]])\n",
            "\n",
            "Self-Attention Output:\n",
            " tensor([[0.3844, 0.6578, 0.6041, 0.5929],\n",
            "        [0.3812, 0.6606, 0.6007, 0.5957],\n",
            "        [0.3815, 0.6546, 0.6349, 0.5934]])\n"
          ]
        }
      ]
    }
  ]
}