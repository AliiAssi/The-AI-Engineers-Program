{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_size, heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.heads = heads\n",
        "        self.head_dim = embed_size // heads\n",
        "\n",
        "        # Linear layers for Q, K, V\n",
        "        self.W_q = nn.Linear(embed_size, embed_size)\n",
        "        self.W_k = nn.Linear(embed_size, embed_size)\n",
        "        self.W_v = nn.Linear(embed_size, embed_size)\n",
        "\n",
        "        # Final output linear layer\n",
        "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
        "\n",
        "    def forward(self, X, mask=None):\n",
        "        batch_size, seq_length, embed_size = X.shape\n",
        "\n",
        "        # Compute Q, K, V\n",
        "        Q, K, V = self.W_q(X), self.W_k(X), self.W_v(X)\n",
        "\n",
        "        # Split into multiple heads and reshape\n",
        "        Q = Q.view(batch_size, seq_length, self.heads, self.head_dim).transpose(1, 2)\n",
        "        K = K.view(batch_size, seq_length, self.heads, self.head_dim).transpose(1, 2)\n",
        "        V = V.view(batch_size, seq_length, self.heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Compute attention scores\n",
        "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "\n",
        "        # Apply causal mask (if provided)\n",
        "        if mask is not None:\n",
        "            attention_scores = attention_scores.masked_fill(mask == 0, float(\"-inf\"))\n",
        "\n",
        "        # Compute attention weights with softmax\n",
        "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
        "\n",
        "        # Apply attention to values\n",
        "        out = torch.matmul(attention_weights, V)\n",
        "\n",
        "        # Merge heads and pass through output layer\n",
        "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_length, embed_size)\n",
        "        out = self.fc_out(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Example input (batch_size=1, seq_length=5, embed_size=8)\n",
        "seq_length = 5\n",
        "embed_size = 8\n",
        "heads = 2\n",
        "\n",
        "X = torch.rand((1, seq_length, embed_size))\n",
        "\n",
        "# Create causal mask (lower triangular matrix)\n",
        "mask = torch.tril(torch.ones((seq_length, seq_length))).unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "# Initialize attention layer and apply it with the mask\n",
        "attention_layer = MultiHeadAttention(embed_size=embed_size, heads=heads)\n",
        "output = attention_layer(X, mask=mask)\n",
        "\n",
        "print(\"Causal Mask:\\n\", mask)\n",
        "print(\"\\nMulti-Head Attention Output with Causal Mask:\\n\", output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7PRzrSiiCO8",
        "outputId": "6af17923-0bfe-4c0d-ef2e-a5562c264d4b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Causal Mask:\n",
            " tensor([[[[1., 0., 0., 0., 0.],\n",
            "          [1., 1., 0., 0., 0.],\n",
            "          [1., 1., 1., 0., 0.],\n",
            "          [1., 1., 1., 1., 0.],\n",
            "          [1., 1., 1., 1., 1.]]]])\n",
            "\n",
            "Multi-Head Attention Output with Causal Mask:\n",
            " tensor([[[-0.4995,  0.0182, -0.0265, -0.2035, -0.2631,  0.3689,  0.1295,\n",
            "          -0.1388],\n",
            "         [-0.4098, -0.0201, -0.0017, -0.2257, -0.2599,  0.3769,  0.0983,\n",
            "          -0.1072],\n",
            "         [-0.4187,  0.0212,  0.0258, -0.2230, -0.2395,  0.3721,  0.1164,\n",
            "          -0.1127],\n",
            "         [-0.4040,  0.0309,  0.0365, -0.2115, -0.2208,  0.3639,  0.1202,\n",
            "          -0.1133],\n",
            "         [-0.4114,  0.0497,  0.0763, -0.2018, -0.2005,  0.3479,  0.1173,\n",
            "          -0.1143]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    }
  ]
}