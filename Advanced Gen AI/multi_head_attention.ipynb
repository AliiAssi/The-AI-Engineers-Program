{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_size, heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.heads = heads\n",
        "        self.head_dim = embed_size // heads\n",
        "\n",
        "        # Linear layers for Q, K, V\n",
        "        self.W_q = nn.Linear(embed_size, embed_size)\n",
        "        self.W_k = nn.Linear(embed_size, embed_size)\n",
        "        self.W_v = nn.Linear(embed_size, embed_size)\n",
        "\n",
        "        # Final output linear layer\n",
        "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
        "\n",
        "    def forward(self, X):\n",
        "        batch_size, seq_length, embed_size = X.shape\n",
        "\n",
        "        # Compute Q, K, V\n",
        "        Q, K, V = self.W_q(X), self.W_k(X), self.W_v(X)\n",
        "\n",
        "        # Split into multiple heads and reshape\n",
        "        Q = Q.view(batch_size, seq_length, self.heads, self.head_dim).transpose(1, 2)\n",
        "        K = K.view(batch_size, seq_length, self.heads, self.head_dim).transpose(1, 2)\n",
        "        V = V.view(batch_size, seq_length, self.heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Compute attention scores\n",
        "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "        attention_weights = torch.nn.functional.softmax(attention_scores, dim=-1)\n",
        "        out = torch.matmul(attention_weights, V)\n",
        "\n",
        "        # Merge heads and pass through output layer\n",
        "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_length, embed_size)\n",
        "        out = self.fc_out(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Example input (batch_size=1, seq_length=3, embed_size=8)\n",
        "X = torch.rand((1, 3, 8))\n",
        "attention_layer = MultiHeadAttention(embed_size=8, heads=2)\n",
        "output = attention_layer(X)\n",
        "\n",
        "print(\"Multi-Head Attention Output:\\n\", output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7PRzrSiiCO8",
        "outputId": "548a7a57-db0b-48ce-a989-7fd4b22ed417"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Weights:\n",
            " tensor([[0.2668, 0.4338, 0.2994],\n",
            "        [0.2735, 0.4257, 0.3008],\n",
            "        [0.2932, 0.4504, 0.2563]])\n",
            "\n",
            "Self-Attention Output:\n",
            " tensor([[0.3844, 0.6578, 0.6041, 0.5929],\n",
            "        [0.3812, 0.6606, 0.6007, 0.5957],\n",
            "        [0.3815, 0.6546, 0.6349, 0.5934]])\n"
          ]
        }
      ]
    }
  ]
}